{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string = \"\"\"You are a master branding consulatant who specializes in naming brands. \\\n",
    "You come up with catchy and memorable brand names.\n",
    "\n",
    "Take the brand description below delimited by triple backticks and use it to create the name for a brand.\n",
    "\n",
    "brand description: ```{brand_description}```\n",
    "\n",
    "then based on the description and you hot new brand name give the brand a score 1-10 for how likely it is to succeed.\n",
    "\"\"\"\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "branding_messages = prompt_template.format_messages( brand_description=\"a cool hip new sneaker brand aimed at rich kids\")\n",
    "consultant_response = chat_llm(branding_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Brand Name: LuxKicks\\n\\nSuccess Score: 8')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consultant_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"ingredients\": string  // The ingredients from recipe, as a unique string.\n",
      "\t\"steps\": string  // The steps to prepare the recipe, as a unique string.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "\n",
    "ingredients = ResponseSchema(\n",
    "        name=\"ingredients\",\n",
    "        description=\"The ingredients from recipe, as a unique string.\",\n",
    "    )\n",
    "steps = ResponseSchema(\n",
    "        name=\"steps\",\n",
    "        description=\"The steps to prepare the recipe, as a unique string.\",\n",
    "    )\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(\n",
    "    [ingredients, steps]\n",
    ")\n",
    "\n",
    "response_format = output_parser.get_format_instructions()\n",
    "print(response_format)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"What is the recipe for {recipe}? Return the ingredients list and steps separately. \\n {format_instructions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open AI:\n",
      "\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"ingredients\": \"For the Fish:  Vegetable oil, for deep-frying  2 large cod fillets  1 cup all-purpose flour  1 teaspoon baking powder  Salt and freshly ground black pepper  For the Chips:  4 large potatoes  Vegetable oil, for deep-frying  Salt\",\n",
      "\t\"steps\": \"1. Heat oil in a deep-fryer or heavy pot to 375 degrees F (190 degrees C). 2. Mix flour, baking powder, salt and pepper together in a bowl. 3. Cut the potatoes into chips. 4. Put the chips into the hot oil, and fry for 5 to 10 minutes, until golden-brown. 5. Take out of the oil with a slotted spoon and put on paper towels to drain. 6. Dust the cod fillets with the seasoned flour, and shake off any excess. 7. Deep-fry the fish in the hot oil for about 3 minutes, until golden-brown and cooked through. 8. Take out of the oil with a slotted spoon and put on paper towels to drain. 9. Serve the fish with the chips.\"\n",
      "}\n",
      "```\n",
      "{'ingredients': 'For the Fish:  Vegetable oil, for deep-frying  2 large cod fillets  1 cup all-purpose flour  1 teaspoon baking powder  Salt and freshly ground black pepper  For the Chips:  4 large potatoes  Vegetable oil, for deep-frying  Salt', 'steps': '1. Heat oil in a deep-fryer or heavy pot to 375 degrees F (190 degrees C). 2. Mix flour, baking powder, salt and pepper together in a bowl. 3. Cut the potatoes into chips. 4. Put the chips into the hot oil, and fry for 5 to 10 minutes, until golden-brown. 5. Take out of the oil with a slotted spoon and put on paper towels to drain. 6. Dust the cod fillets with the seasoned flour, and shake off any excess. 7. Deep-fry the fish in the hot oil for about 3 minutes, until golden-brown and cooked through. 8. Take out of the oil with a slotted spoon and put on paper towels to drain. 9. Serve the fish with the chips.'}\n"
     ]
    }
   ],
   "source": [
    "llm_openai = OpenAI()\n",
    "#llm_palm = GooglePalm()\n",
    "\n",
    "recipe = 'Fish and chips'\n",
    "\n",
    "formated_prompt = prompt.format(**{\"recipe\":recipe, \"format_instructions\":output_parser.get_format_instructions()})\n",
    "\n",
    "# response_palm = llm_palm(formated_prompt)\n",
    "response_openai = llm_openai(formated_prompt)\n",
    "\n",
    "# print(\"PaLM:\")\n",
    "# print(response_palm)\n",
    "# print(output_parser.parse(response_palm))\n",
    "\n",
    "print(\"Open AI:\")\n",
    "print(response_openai)\n",
    "print(output_parser.parse(response_openai))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ingredients': 'For the Fish:  Vegetable oil, for deep-frying  2 large cod fillets  1 cup all-purpose flour  1 teaspoon baking powder  Salt and freshly ground black pepper  For the Chips:  4 large potatoes  Vegetable oil, for deep-frying  Salt', 'steps': '1. Heat oil in a deep-fryer or heavy pot to 375 degrees F (190 degrees C). 2. Mix flour, baking powder, salt and pepper together in a bowl. 3. Cut the potatoes into chips. 4. Put the chips into the hot oil, and fry for 5 to 10 minutes, until golden-brown. 5. Take out of the oil with a slotted spoon and put on paper towels to drain. 6. Dust the cod fillets with the seasoned flour, and shake off any excess. 7. Deep-fry the fish in the hot oil for about 3 minutes, until golden-brown and cooked through. 8. Take out of the oil with a slotted spoon and put on paper towels to drain. 9. Serve the fish with the chips.'}\n"
     ]
    }
   ],
   "source": [
    "json_output = output_parser.parse(response_openai)\n",
    "print(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Heat oil in a deep-fryer or heavy pot to 375 degrees F (190 degrees C). 2. Mix flour, baking powder, salt and pepper together in a bowl. 3. Cut the potatoes into chips. 4. Put the chips into the hot oil, and fry for 5 to 10 minutes, until golden-brown. 5. Take out of the oil with a slotted spoon and put on paper towels to drain. 6. Dust the cod fillets with the seasoned flour, and shake off any excess. 7. Deep-fry the fish in the hot oil for about 3 minutes, until golden-brown and cooked through. 8. Take out of the oil with a slotted spoon and put on paper towels to drain. 9. Serve the fish with the chips.\n"
     ]
    }
   ],
   "source": [
    "print(json_output['steps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"task_description\": string  // The description of each task, with each description stored as a unique string in the list.\n",
      "\t\"deadline\": string  // The deadline of the corresponding task_description, stored as a unqiue string in the list, in the same order as it appears in task_description.\n",
      "\t\"person_in_charge\": string  // The people in involved in each task, who needs to perform follow-up actions after the meeting, stored as a unqiue string in the list, in the same order as it appears in task_description.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "\n",
    "main_topics = ResponseSchema(\n",
    "        name=\"task_description\",\n",
    "        description=\"The description of each task, with each description stored as a unique string in the list.\",\n",
    "    )\n",
    "deadline = ResponseSchema(\n",
    "        name=\"deadline\",\n",
    "        description=\"The deadline of the corresponding task_description, stored as a unqiue string in the list, in the same order as it appears in task_description.\",\n",
    "    )\n",
    "\n",
    "person_in_charge = ResponseSchema(\n",
    "        name=\"person_in_charge\",\n",
    "        description=\"The people in involved in each task, who needs to perform follow-up actions after the meeting, stored as a unqiue string in the list, in the same order as it appears in task_description.\",\n",
    "    )\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(\n",
    "    [main_topics, deadline, person_in_charge]\n",
    ")\n",
    "\n",
    "response_format = output_parser.get_format_instructions()\n",
    "print(response_format)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Summarise the meeting transcript by providing a short description up to 50 words for each task mentioned, the deadline of each task and the person in charge to follow-up on it after the meeting. The meeting transcript is delimited with triple backticks. Return the task_description, deadline and person_in_charge seperately. '''{meeting_transcript}''' \\n {format_instructions}\")\n",
    "\n",
    "\n",
    "llm_openai = OpenAI()\n",
    "with open('transcripts/meeting002.txt', 'r') as file:\n",
    "    data = file.read()\n",
    "meeting_transcript = data\n",
    "\n",
    "formated_prompt = prompt.format(**{\"meeting_transcript\":meeting_transcript, \"format_instructions\":output_parser.get_format_instructions()})\n",
    "response_openai = llm_openai(formated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"task_description\": [\"Research the cost of implementing the green features\", \"Research vendors who can provide the green features\"],\n",
      "\t\"deadline\": [\"2 weeks\", \"16 Oct\"],\n",
      "\t\"person_in_charge\": [\"Jane and Mary\", \"John\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response_openai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"task_description\": [\"Research the cost of implementing the green features\", \"Research vendors who can provide the green features\"],\n",
      "\t\"deadline\": [\"2 weeks\", \"16 Oct\"],\n",
      "\t\"person_in_charge\": [\"Jane and Mary\", \"John\"]\n",
      "}\n",
      "```\n",
      "{'task_description': ['Research the cost of implementing the green features', 'Research vendors who can provide the green features'], 'deadline': ['2 weeks', '16 Oct'], 'person_in_charge': ['Jane and Mary', 'John']}\n"
     ]
    }
   ],
   "source": [
    "print(response_openai)\n",
    "print(output_parser.parse(response_openai))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca0d0e525f54b998c2562dbc09f4f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4ddfeb1c0246beba6762c6f6b2545c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "# begin initializing HF items, you need an access token\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,)\n",
    "\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "\n",
    "# enable evaluation mode to allow model inference\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    ")\n",
    "\n",
    "stop_list = ['\\nHuman:', '\\n```\\n']\n",
    "\n",
    "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
    "stop_token_ids\n",
    "\n",
    "import torch\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "stop_token_ids\n",
    "\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n",
    "\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mldadmin/home/s123mdg310_03/miniconda3/envs/conda_env1/lib/python3.11/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/media/mldadmin/home/s123mdg310_03/miniconda3/envs/conda_env1/lib/python3.11/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "\"meeting_date\": \"2023-10-13\",\n",
      "\"attendees_list\": [\"John Doe\", \"Jane Smith\", \"Mary Green\"],\n",
      "\"start_time\": \"11:27 PST\",\n",
      "\"end_time\": \"14:27 PST\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "text_chunks = text_splitter.split_documents(TextLoader(\"transcripts/meeting002.txt\").load())\n",
    "model_name = [\"sentence-transformers/all-mpnet-base-v2\", 'sentence-transformers/all-MiniLM-L6-v2', \"sentence-transformers/paraphrase-MiniLM-L6-v2\"]\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name[1],model_kwargs={'device': 'cuda'})\n",
    "vectorstore=FAISS.from_documents(text_chunks, embeddings)\n",
    "\n",
    "\n",
    "llm=HuggingFacePipeline(pipeline=generate_text, model_kwargs={'temperature':0})\n",
    "chain =  RetrievalQA.from_chain_type(llm=llm, chain_type = \"stuff\",return_source_documents=False, retriever=vectorstore.as_retriever())\n",
    "query_list = [\"extract the following information from the meeting transcript: 1) meeting_date: what is the date of the meeting in DD/MM/YYYY format? 2) attendees_list: what is the full name of everyone who attended the meeting? 3)start_time: when did the meeting started? 4) end_time: when did the meeting ended? You should format your response as JSON with the following keys: meeting_date, attendees_list, start_time, end_time\"]\n",
    "chat_history = []\n",
    "for query in query_list:\n",
    "    result = chain({\"query\": query, \"chat_history\": chat_history})\n",
    "    #chat_history.append((query, result[\"result\"].strip(\"\\n\")))\n",
    "    result=chain({\"query\": query, \"chat_history\": []},return_only_outputs=True)\n",
    "    print(result['result'])\n",
    "meeting_info = result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"meeting_date\": string  // date of the meeting stored in datetime format DD/MM/YYYY.\n",
      "\t\"attendees_list\": string  // Full name of everyone present in the meeting, each stored as a string in the list.\n",
      "\t\"start_time\": string  // time the meeting started in 24 hour HH:mm format in datetime\n",
      "\t\"end_time\": string  // time the meeting ended in 24 hour HH:mm format in datetime\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"meeting_date\": \"2023-10-13\",\n",
      "\t\"attendees_list\": [\"John Doe\", \"Jane Smith\", \"Mary Green\"],\n",
      "\t\"start_time\": \"11:27 PST\",\n",
      "\t\"end_time\": \"14:27 PST\"\n",
      "}\n",
      "```\n",
      "printing response\n",
      "{'meeting_date': '2023-10-13', 'attendees_list': ['John Doe', 'Jane Smith', 'Mary Green'], 'start_time': '11:27 PST', 'end_time': '14:27 PST'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "\n",
    "meeting_date = ResponseSchema(\n",
    "        name=\"meeting_date\",\n",
    "        description=\"date of the meeting stored in datetime format DD/MM/YYYY.\",\n",
    "    )\n",
    "attendees_list = ResponseSchema(\n",
    "        name=\"attendees_list\",\n",
    "        description=\"Full name of everyone present in the meeting, each stored as a string in the list.\",\n",
    "    )\n",
    "\n",
    "start_time = ResponseSchema(\n",
    "        name=\"start_time\",\n",
    "        description=\"time the meeting started in 24 hour HH:mm format in datetime\",\n",
    "    )\n",
    "\n",
    "end_time = ResponseSchema(\n",
    "        name=\"end_time\",\n",
    "        description=\"time the meeting ended in 24 hour HH:mm format in datetime\",\n",
    "    )\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(\n",
    "    [meeting_date, attendees_list, start_time, end_time]\n",
    ")\n",
    "\n",
    "response_format = output_parser.get_format_instructions()\n",
    "print(response_format)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"You are a helpful formatting assistant. Return the meeting_date, attendees_list, start_time, end_time separately. '''{meeting_info}''' \\n {format_instructions}\")\n",
    "\n",
    "llm_openai = OpenAI()\n",
    "formated_prompt = prompt.format(**{\"meeting_info\":meeting_info, \"format_instructions\":output_parser.get_format_instructions()})\n",
    "response_openai = llm_openai(formated_prompt)\n",
    "print(response_openai)\n",
    "print('printing response')\n",
    "meeting_info_dict = output_parser.parse(response_openai)\n",
    "print(meeting_info_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mldadmin/home/s123mdg310_03/miniconda3/envs/conda_env1/lib/python3.11/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mldadmin/home/s123mdg310_03/miniconda3/envs/conda_env1/lib/python3.11/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "\"task_description\": [\n",
      "\"Research the cost of implementing the green features\",\n",
      "\"Update the team in 2 weeks\",\n",
      "\"Research vendors who can provide these green features\",\n",
      "\"Update you guys by 16 Oct\"\n",
      "],\n",
      "\"deadline\": \"2 weeks\",\n",
      "\"people_in_charge\": {\n",
      "\"green building design concept\": [\"Jane Smith\", \"Mary Green\"],\n",
      "\"cost of implementing green features\": [\"Jane Smith\", \"John Doe\"]\n",
      "}\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mldadmin/home/s123mdg310_03/miniconda3/envs/conda_env1/lib/python3.11/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/media/mldadmin/home/s123mdg310_03/miniconda3/envs/conda_env1/lib/python3.11/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure! Based on the transcript provided, here is a meeting summary in prose:\n",
      "\n",
      "The meeting began with John Doe, the Design Lead, welcoming everyone to the discussion on the green building design concept developed by Jane Smith. Jane presented her concept, which included maximizing natural light and ventilation, using renewable energy sources, such as solar panels and wind turbines, utilizing sustainable materials, such as recycled wood and bamboo, and incorporating water conservation features, such as rainwater harvesting and low-flow toilets.\n",
      "\n",
      "John expressed his concern about the cost of implementing all of these green features, but Jane addressed this by pointing out the potential long-term savings, including reduced reliance on the grid, lower water bills, and longer-lasting materials. To further investigate the cost of these green features, John assigned Jane and Mary Green to research the cost of implementing the proposed green features within two weeks. He also took responsibility for researching vendors who could provide these green features and updated the team by October 16th.\n",
      "\n",
      "In summary, the tasks assigned were:\n",
      "\n",
      "* Researching the cost of implementing green features: Jane and Mary Green (due within 2 weeks)\n",
      "* Researching vendors for green features: John Doe (by October 16th)\n",
      "\n",
      "The people in charge of each task are:\n",
      "\n",
      "* Jane Smith: Researching cost of green features\n",
      "* Mary Green: Researching cost of green features\n",
      "* John Doe: Researching vendors for green features\n",
      "\n",
      "The deadlines for each task are:\n",
      "\n",
      "* Jane and Mary: Within 2 weeks\n",
      "* John: By October 16th\n"
     ]
    }
   ],
   "source": [
    "llm=HuggingFacePipeline(pipeline=generate_text, model_kwargs={'temperature':0})\n",
    "chain =  RetrievalQA.from_chain_type(llm=llm, chain_type = \"stuff\",return_source_documents=False, retriever=vectorstore.as_retriever())\n",
    "query_list = [ \"you are a helpful AI meeting minute writer. Help me to write a meeting summary in prose, detailing all the important tasks mentioned, especially the people in charge of each task and the respective deadlines if any. Your minute should be ordered based on the topics discussed and not on chronological order.\"]\n",
    "chat_history = []\n",
    "for query in query_list:\n",
    "    result = chain({\"query\": query, \"chat_history\": chat_history})\n",
    "    #chat_history.append((query, result[\"result\"].strip(\"\\n\")))\n",
    "    result=chain({\"query\": query, \"chat_history\": []},return_only_outputs=True)\n",
    "    print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary in prose:\n",
      "\n",
      "The meeting began with John Doe, the Design Lead, welcoming everyone to the discussion on the green building design concept developed by Jane Smith. Jane presented her concept, which included maximizing natural light and ventilation, using renewable energy sources, such as solar panels and wind turbines, utilizing sustainable materials, such as recycled wood and bamboo, and incorporating water conservation features, such as rainwater harvesting and low-flow toilets.\n",
      "\n",
      "John expressed his concern about the cost of implementing all of these green features, but Jane addressed this by pointing out the potential long-term savings, including reduced reliance on the grid, lower water bills, and longer-lasting materials. To further investigate the cost of these green features, John assigned Jane and Mary Green to research the cost of implementing the proposed green features within two weeks. He also took responsibility for researching vendors who could provide these green features and updated the team by October 16th.\n",
      "\n",
      "In summary, the tasks assigned were:\n",
      "\n",
      "* Researching the cost of implementing green features: Jane and Mary Green (due within 2 weeks)\n",
      "* Researching vendors for green features: John Doe (by October 16th)\n",
      "\n",
      "The people in charge of each task are:\n",
      "\n",
      "* Jane Smith: Researching cost of green features\n",
      "* Mary Green: Researching cost of green features\n",
      "* John Doe: Researching vendors for green features\n",
      "\n",
      "The deadlines for each task are:\n",
      "\n",
      "* Jane and Mary: Within 2 weeks\n",
      "* John: By October 16th\n"
     ]
    }
   ],
   "source": [
    "meeting = result['result'][result['result'].find('summary'):]\n",
    "print(meeting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "meeting = \"The meeting began with John Doe, the Design Lead, welcoming everyone to the discussion on the green building design concept developed by Jane Smith. Jane presented her concept, which included maximizing natural light and ventilation, using renewable energy sources, such as solar panels and wind turbines, utilizing sustainable materials, such as recycled wood and bamboo, and incorporating water conservation features, such as rainwater harvesting and low-flow toilets. \\n John expressed his concern about the cost of implementing all of these green features, but Jane addressed this by pointing out the potential long-term savings, including reduced reliance on the grid, lower water bills, and longer-lasting materials. To further investigate the cost of these green features, John assigned Jane and Mary Green to research the cost of implementing the proposed green features within two weeks. He also took responsibility for researching vendors who could provide these green features and updated the team by October 16th.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"task_description\": string  // Describe each task in detail and store it as a string in the list.\n",
      "\t\"deadline\": string  // The deadline of the corresponding task_description, stored as a unique string in the list, in the same order as it appears in task_description.\n",
      "\t\"person_in_charge\": string  // default str is NA. The people in involved in each task, who needs to perform follow-up actions after the meeting or had presented this task in the meeting, stored as a string in the list, in the same order as it appears in task_description.\n",
      "\t\"days_left\": string  // Optional parameter, default value is int 0. If the deadline given is relative from the date of the meeting, store as an int (the number of days given) in the list, in the same order as it appears in task_description.\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "```\n",
      "<class 'str'>\n",
      "\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"task_description\": [\"Jane presented her concept, which included maximizing natural light and ventilation, using renewable energy sources, such as solar panels and wind turbines, utilizing sustainable materials, such as recycled wood and bamboo, and incorporating water conservation features, such as rainwater harvesting and low-flow toilets.\", \"To further investigate the cost of these green features, Jane and Mary Green were assigned to research the cost of implementing the proposed green features within two weeks.\", \"John took responsibility for researching vendors who could provide these green features and updated the team by October 16th.\"], \n",
      "  \"deadline\": [\"two weeks\", \"October 16th\"], \n",
      "  \"person_in_charge\": [\"Jane and Mary Green\", \"John\"], \n",
      "  \"days_left\": [14, null]\n",
      "}\n",
      "```\n",
      "printing response\n",
      "{'task_description': ['Jane presented her concept, which included maximizing natural light and ventilation, using renewable energy sources, such as solar panels and wind turbines, utilizing sustainable materials, such as recycled wood and bamboo, and incorporating water conservation features, such as rainwater harvesting and low-flow toilets.', 'To further investigate the cost of these green features, Jane and Mary Green were assigned to research the cost of implementing the proposed green features within two weeks.', 'John took responsibility for researching vendors who could provide these green features and updated the team by October 16th.'], 'deadline': ['two weeks', 'October 16th'], 'person_in_charge': ['Jane and Mary Green', 'John'], 'days_left': [14, None]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "\n",
    "main_topics = ResponseSchema(\n",
    "        name=\"task_description\",\n",
    "        description=\"Describe each task in detail and store it as a string in the list.\",\n",
    "    )\n",
    "deadline = ResponseSchema(\n",
    "        name=\"deadline\",\n",
    "        description=\"The deadline of the corresponding task_description, stored as a unique string in the list, in the same order as it appears in task_description.\",\n",
    "    )\n",
    "\n",
    "person_in_charge = ResponseSchema(\n",
    "        name=\"person_in_charge\",\n",
    "        description=\"default str is NA. The people in involved in each task, who needs to perform follow-up actions after the meeting or had presented this task in the meeting, stored as a string in the list, in the same order as it appears in task_description.\",\n",
    "    )\n",
    "\n",
    "days_left = ResponseSchema(\n",
    "        name=\"days_left\",\n",
    "        description=\"Optional parameter, default value is int 0. If the deadline given is relative from the date of the meeting, store as an int (the number of days given) in the list, in the same order as it appears in task_description.\",\n",
    "    )\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(\n",
    "    [main_topics, deadline, person_in_charge, days_left]\n",
    ")\n",
    "\n",
    "response_format = output_parser.get_format_instructions()\n",
    "print(response_format)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"You are a helpful formatting assistant. The meeting transcript is delimited with triple backticks. It is already a summarised version, hence you don't need to summarise it further when providing task_description. A task can be what team members have presented in the meeting, or follow-up actions required after this meeting. Return the task_description, and their respective deadline, person_in_charge and days_left separately. '''{meeting_transcript}''' \\n {format_instructions}\")\n",
    "\n",
    "\n",
    "llm_openai = OpenAI()\n",
    "with open('transcripts/meeting002.txt', 'r') as file:\n",
    "    data = file.read()\n",
    "meeting_transcript = meeting\n",
    "\n",
    "formated_prompt = prompt.format(**{\"meeting_transcript\":meeting_transcript, \"format_instructions\":output_parser.get_format_instructions()})\n",
    "response_openai = llm_openai(formated_prompt)\n",
    "print(response_openai[:5])\n",
    "print(type(response_openai))\n",
    "print(response_openai)\n",
    "\n",
    "if response_openai[0] == '[':\n",
    "    print('error in',response_openai )\n",
    "    response_openai = response_openai[1:-2]\n",
    "    print('re-attempt with',response_openai )\n",
    "print('printing response')\n",
    "store_result = output_parser.parse(response_openai)\n",
    "print(store_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_response = {'task_description': ['Jane presented her concept, which included maximizing natural light and ventilation, using renewable energy sources, such as solar panels and wind turbines, utilizing sustainable materials, such as recycled wood and bamboo, and incorporating water conservation features, such as rainwater harvesting and low-flow toilets.', 'To further investigate the cost of these green features, Jane and Mary Green were assigned to research the cost of implementing the proposed green features within two weeks.', 'John took responsibility for researching vendors who could provide these green features and updated the team by October 16th.'], 'deadline': ['two weeks', 'October 16th'], 'person_in_charge': ['Jane and Mary Green', 'John'], 'days_left': [14, None]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"deadline\": string  // The deadline of the corresponding task_description, stored as a datetime format in the list, in the same order as it appears in task_description.\n",
      "\t\"person_in_charge\": string  // default str is NA. The people in involved in each task, who needs to perform follow-up actions after the meeting or had presented this task in the meeting, stored as a string in the list, in the same order as it appears in task_description.\n",
      "\t\"days_left\": string  // Optional parameter, default value is int 0. If the deadline given is relative from the date of the meeting, store as an int (the number of days given) in the list, in the same order as it appears in task_description.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(\n",
    "    [deadline, person_in_charge, days_left]\n",
    ")\n",
    "\n",
    "response_format = output_parser.get_format_instructions()\n",
    "print(response_format)\n",
    "\n",
    "task_output = []\n",
    "for task in best_response['task_description']:\n",
    "    prompt = ChatPromptTemplate.from_template(\" The task denoted by double backticks, ``{task}`` is mentioned in the meeting. The meeting_transcript is delimited by triple quotes. What is the deadline, person_in_charge and days_left for this task? Return the deadline, person_in_charge and days_left separately. '''{meeting_transcript}''' \\n {format_instructions}\")\n",
    "\n",
    "\n",
    "    llm_openai = OpenAI()\n",
    "    with open('transcripts/meeting002.txt', 'r') as file:\n",
    "        data = file.read()\n",
    "    meeting_transcript = meeting\n",
    "\n",
    "    formated_prompt = prompt.format(**{\"meeting_transcript\":meeting_transcript, \"task\":task, \"format_instructions\":output_parser.get_format_instructions()})\n",
    "    response_openai = llm_openai(formated_prompt)\n",
    "    store_result = output_parser.parse(response_openai)\n",
    "    task_output.append(store_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'deadline': 'October 16th', 'person_in_charge': 'Jane Smith and Mary Green', 'days_left': 14}, {'deadline': 'October 16th', 'person_in_charge': 'Jane and Mary Green', 'days_left': 'two weeks'}, {'deadline': 'October 16th', 'person_in_charge': 'John', 'days_left': 'NA'}]\n"
     ]
    }
   ],
   "source": [
    "print(task_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['October 16th', 'Jane Smith and Mary Green'], ['October 16th', 'Jane and Mary Green'], ['October 16th', 'John']]\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta, date\n",
    "task_output_reformatted = []\n",
    "for task_out in task_output:\n",
    "    raw_list = list(task_out.values())\n",
    "    final_list = [raw_list[0] if raw_list[0] != 'NA' else date.today() + timedelta(days=raw_list[2]), raw_list[1]]\n",
    "    task_output_reformatted.append(final_list)\n",
    "print(task_output_reformatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'task_description': 'Jane presented her concept, which included maximizing natural light and ventilation, using renewable energy sources, such as solar panels and wind turbines, utilizing sustainable materials, such as recycled wood and bamboo, and incorporating water conservation features, such as rainwater harvesting and low-flow toilets.', 'additional_info': ['October 16th', 'Jane Smith and Mary Green']}, {'task_description': 'To further investigate the cost of these green features, Jane and Mary Green were assigned to research the cost of implementing the proposed green features within two weeks.', 'additional_info': ['October 16th', 'Jane and Mary Green']}, {'task_description': 'John took responsibility for researching vendors who could provide these green features and updated the team by October 16th.', 'additional_info': ['October 16th', 'John']}]\n"
     ]
    }
   ],
   "source": [
    "table_content = []\n",
    "for i, task in enumerate(best_response['task_description']):\n",
    "    row_dict = {'task_description': task, 'additional_info': task_output_reformatted[i]}\n",
    "    table_content.append(row_dict)\n",
    "print(table_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meeting_date': '2023-10-13',\n",
       " 'attendees_list': ['John Doe', 'Jane Smith', 'Mary Green'],\n",
       " 'start_time': '11:27 PST',\n",
       " 'end_time': '14:27 PST'}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meeting_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meeting_date': '2023-10-13', 'attendees_list': 'John Doe\\nJane Smith\\nMary Green', 'start_time': '11:27 PST', 'end_time': '14:27 PST', 'col_labels': ['Deadline', 'Person-In-Charge'], 'tbl_contents': [{'task_description': 'Jane presented her concept, which included maximizing natural light and ventilation, using renewable energy sources, such as solar panels and wind turbines, utilizing sustainable materials, such as recycled wood and bamboo, and incorporating water conservation features, such as rainwater harvesting and low-flow toilets.', 'additional_info': ['October 16th', 'Jane Smith and Mary Green']}, {'task_description': 'To further investigate the cost of these green features, Jane and Mary Green were assigned to research the cost of implementing the proposed green features within two weeks.', 'additional_info': ['October 16th', 'Jane and Mary Green']}, {'task_description': 'John took responsibility for researching vendors who could provide these green features and updated the team by October 16th.', 'additional_info': ['October 16th', 'John']}]}\n"
     ]
    }
   ],
   "source": [
    "context = meeting_info_dict.copy()\n",
    "context['attendees_list'] =  '\\n'.join(meeting_info_dict['attendees_list'])\n",
    "context['col_labels'] = ['Deadline', 'Person-In-Charge']\n",
    "context['tbl_contents'] = table_content\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docxtpl import DocxTemplate\n",
    "from datetime import  datetime\n",
    "template = DocxTemplate(\"dynamic_table.docx\")\n",
    "\n",
    "template.render(test)\n",
    "template.save(\"dynamic_table_out.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid\n"
     ]
    }
   ],
   "source": [
    "mystr = '{[{ddfd]sdf}gdgff]}'\n",
    "\n",
    "def valid_brackets(mystr):\n",
    "    if mystr[::-1].index(']') < mystr[::-1].index('}') and mystr.index('[') < mystr.index('{'):\n",
    "        print('string is invalid')\n",
    "        newstr = mystr[:mystr.index('[')] + mystr[mystr.index('[')+1:len(mystr)-mystr[::-1].index(']')-1] + mystr[len(mystr)-mystr[::-1].index(']'):]\n",
    "        print(newstr)\n",
    "    else:\n",
    "        print('valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how a valid response_openai is formatted \n",
    "\n",
    "```json\n",
    "{\n",
    "        \"task_description\": [\"Jane and Mary to research implementation costs within two weeks\", \"John to look into finding suitable vendors for the green features\"],\n",
    "        \"deadline\": [\"October 16th\", \"October 16th\"],\n",
    "        \"person_in_charge\": [\"Jane and Mary\", \"John\"],\n",
    "        \"days_left\": [14, 14]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meeting_date': '2023-10-13', 'attendees_list': 'John Doe\\nJane Smith\\nMary Green', 'start_time': '11:27 PST', 'end_time': '14:27 PST', 'col_labels': ['Deadline', 'Person-In-Charge'], 'tbl_contents': [{'task_description': 'Jane presented her concept, which included maximizing natural light and ventilation, using renewable energy sources, such as solar panels and wind turbines, utilizing sustainable materials, such as recycled wood and bamboo, and incorporating water conservation features, such as rainwater harvesting and low-flow toilets.', 'additional_info': ['October 16th', 'Jane Smith and Mary Green']}, {'task_description': 'To further investigate the cost of these green features, Jane and Mary Green were assigned to research the cost of implementing the proposed green features within two weeks.', 'additional_info': ['October 16th', 'Jane and Mary Green']}, {'task_description': 'John took responsibility for researching vendors who could provide these green features and updated the team by October 16th.', 'additional_info': ['October 16th', 'John']}]}\n"
     ]
    }
   ],
   "source": [
    "context = {'meeting_date': '2023-10-13', 'attendees_list': 'John Doe\\nJane Smith\\nMary Green', 'start_time': '11:27 PST', 'end_time': '14:27 PST', 'col_labels': ['Deadline', 'Person-In-Charge'], 'tbl_contents': [{'task_description': 'Jane presented her concept, which included maximizing natural light and ventilation, using renewable energy sources, such as solar panels and wind turbines, utilizing sustainable materials, such as recycled wood and bamboo, and incorporating water conservation features, such as rainwater harvesting and low-flow toilets.', 'additional_info': ['October 16th', 'Jane Smith and Mary Green']}, {'task_description': 'To further investigate the cost of these green features, Jane and Mary Green were assigned to research the cost of implementing the proposed green features within two weeks.', 'additional_info': ['October 16th', 'Jane and Mary Green']}, {'task_description': 'John took responsibility for researching vendors who could provide these green features and updated the team by October 16th.', 'additional_info': ['October 16th', 'John']}]}\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "The string does not contain all `[]{}`.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def str_contains_brackets(str1):\n",
    "  value = min(str1.find('{'), str1.find('}'),str1.find('['),str1.find(']'))\n",
    "  print(value)\n",
    "  return value != -1\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "string = \"This string contains [{].\"\n",
    "if str_contains_brackets(string):\n",
    "  print(\"The string contains all `[]{}`.\")\n",
    "else:\n",
    "  print(\"The string does not contain all `[]{}`.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
