{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sg_parliament_4h.mp3\n",
      "sg_parliament_20min.mp3\n",
      "sg_documentary_45min.mp3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"../sample_audio\"  # Replace with the actual path to your folder\n",
    "\n",
    "# Get a list of all files in the folder\n",
    "file_list = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, file))]\n",
    "\n",
    "# Print the list of file names relative to the root directory\n",
    "for file in file_list:\n",
    "    print(os.path.relpath(file, folder_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get the directory of the current script\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m script_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Construct the path to the audio file\u001b[39;00m\n\u001b[1;32m      5\u001b[0m audio_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(script_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_audio\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msg_parliament_20min.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "audio_file = \"../sample_audio/sg_parliament_20min.mp3\"\n",
    "\n",
    "if os.path.exists(audio_file):\n",
    "    print(\"File exists.\")\n",
    "else:\n",
    "    print(\"File does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.2.0+cu121. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Library libcublas.so.11 is not found or cannot be loaded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 24\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# save model to local path (optional)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# model_dir = \"/path/\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type, download_root=model_dir)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m audio \u001b[38;5;241m=\u001b[39m whisperx\u001b[38;5;241m.\u001b[39mload_audio(audio_file)\n\u001b[0;32m---> 24\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegments\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;66;03m# before alignment\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# delete model if low on GPU resources\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# import gc; gc.collect(); torch.cuda.empty_cache(); del model\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 2. Align whisper output\u001b[39;00m\n",
      "File \u001b[0;32m~/Convo2Calendar/whisperX/whisperx/asr.py:194\u001b[0m, in \u001b[0;36mFasterWhisperPipeline.transcribe\u001b[0;34m(self, audio, batch_size, num_workers, language, task, chunk_size, print_progress, combined_progress)\u001b[0m\n\u001b[1;32m    187\u001b[0m vad_segments \u001b[38;5;241m=\u001b[39m merge_chunks(\n\u001b[1;32m    188\u001b[0m     vad_segments,\n\u001b[1;32m    189\u001b[0m     chunk_size,\n\u001b[1;32m    190\u001b[0m     onset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vad_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvad_onset\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    191\u001b[0m     offset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vad_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvad_offset\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    192\u001b[0m )\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 194\u001b[0m     language \u001b[38;5;241m=\u001b[39m language \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_language\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     task \u001b[38;5;241m=\u001b[39m task \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranscribe\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m faster_whisper\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mTokenizer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mhf_tokenizer,\n\u001b[1;32m    197\u001b[0m                                                         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mis_multilingual, task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[1;32m    198\u001b[0m                                                         language\u001b[38;5;241m=\u001b[39mlanguage)\n",
      "File \u001b[0;32m~/Convo2Calendar/whisperX/whisperx/asr.py:252\u001b[0m, in \u001b[0;36mFasterWhisperPipeline.detect_language\u001b[0;34m(self, audio)\u001b[0m\n\u001b[1;32m    248\u001b[0m model_n_mels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfeat_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    249\u001b[0m segment \u001b[38;5;241m=\u001b[39m log_mel_spectrogram(audio[: N_SAMPLES],\n\u001b[1;32m    250\u001b[0m                               n_mels\u001b[38;5;241m=\u001b[39mmodel_n_mels \u001b[38;5;28;01mif\u001b[39;00m model_n_mels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m80\u001b[39m,\n\u001b[1;32m    251\u001b[0m                               padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m audio\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m N_SAMPLES \u001b[38;5;28;01melse\u001b[39;00m N_SAMPLES \u001b[38;5;241m-\u001b[39m audio\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 252\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdetect_language(encoder_output)\n\u001b[1;32m    254\u001b[0m language_token, language_probability \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Convo2Calendar/whisperX/whisperx/asr.py:86\u001b[0m, in \u001b[0;36mWhisperModel.encode\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     83\u001b[0m     features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(features, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     84\u001b[0m features \u001b[38;5;241m=\u001b[39m faster_whisper\u001b[38;5;241m.\u001b[39mtranscribe\u001b[38;5;241m.\u001b[39mget_ctranslate2_storage(features)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_cpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mto_cpu\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Library libcublas.so.11 is not found or cannot be loaded"
     ]
    }
   ],
   "source": [
    "import whisperx\n",
    "import gc \n",
    "\n",
    "with open(r'../keys/hugging_face.txt', 'r') as fp:\n",
    "    # read all lines using readline()\n",
    "    lines = fp.readlines()\n",
    "    for line in lines:\n",
    "        HF_TOKEN = line\n",
    "\n",
    "\n",
    "device = \"cuda\" \n",
    "audio_file = \"../sample_audio/sg_parliament_20min.mp3\"\n",
    "batch_size = 16 # reduce if low on GPU mem\n",
    "compute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
    "\n",
    "# 1. Transcribe with original whisper (batched)\n",
    "model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type)\n",
    "\n",
    "# save model to local path (optional)\n",
    "# model_dir = \"/path/\"\n",
    "# model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type, download_root=model_dir)\n",
    "\n",
    "audio = whisperx.load_audio(audio_file)\n",
    "result = model.transcribe(audio, batch_size=batch_size)\n",
    "print(result[\"segments\"]) # before alignment\n",
    "\n",
    "# delete model if low on GPU resources\n",
    "# import gc; gc.collect(); torch.cuda.empty_cache(); del model\n",
    "\n",
    "# 2. Align whisper output\n",
    "model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
    "result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "\n",
    "print(result[\"segments\"]) # after alignment\n",
    "\n",
    "# delete model if low on GPU resources\n",
    "# import gc; gc.collect(); torch.cuda.empty_cache(); del model_a\n",
    "\n",
    "# 3. Assign speaker labels\n",
    "diarize_model = whisperx.DiarizationPipeline(use_auth_token=HF_TOKEN, device=device)\n",
    "\n",
    "# add min/max number of speakers if known\n",
    "diarize_segments = diarize_model(audio)\n",
    "# diarize_model(audio, min_speakers=min_speakers, max_speakers=max_speakers)\n",
    "\n",
    "result = whisperx.assign_word_speakers(diarize_segments, result)\n",
    "print(diarize_segments)\n",
    "print(result[\"segments\"]) # segments are now assigned speaker IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/mldadmin/home/s123mdg310_03/Convo2Calendar/scripts\n"
     ]
    }
   ],
   "source": [
    "whisperx examples/sample01.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for s123mdg310_03: \n"
     ]
    }
   ],
   "source": [
    "!sudo apt install libcublas11"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
